# Databricks notebook source
# MAGIC %md
# MAGIC ---
# MAGIC # üìä √âtape 4 : Calcul des KPI de maintenance
# MAGIC
# MAGIC Maintenant que nos donn√©es sont nettoy√©es et unifi√©es, calculons les indicateurs cl√©s de performance (KPI) essentiels pour le pilotage de la maintenance.
# MAGIC
# MAGIC ## üéØ KPI que nous allons calculer :
# MAGIC * **Taux de disponibilit√©** des √©quipements
# MAGIC * **MTBF** (Mean Time Between Failures)
# MAGIC * **MTTR** (Mean Time To Repair)
# MAGIC * **Co√ªts de maintenance** par √©quipement et par type
# MAGIC * **Performance des techniciens**
# MAGIC * **Respect des d√©lais** de maintenance

# COMMAND ----------

catalog = 'gmao_catalog'
schema = 'gmao_schema'
volume = 'gmao_volume'
data_path = f"/Volumes/{catalog}/{schema}/{volume}"

# COMMAND ----------

spark.sql(f"USE CATALOG `{catalog}`")
spark.sql(f"USE SCHEMA `{schema}`")
print(f"‚úÖ Using catalog: {catalog}")
print(f"‚úÖ Using schema: {schema}")

# COMMAND ----------

# DBTITLE 1,üìà KPI 1 : Taux de respect des d√©lais
import matplotlib.pyplot as plt# Calcul du taux de respect des d√©lais

print("üìà KPI 1 : TAUX DE RESPECT DES D√âLAIS")
print("=" * 45)

# Calcul global du taux de respect des d√©lais
delay_kpi = spark.sql("""
    SELECT 
        COUNT(*) as total_ordres_termines,
        SUM(CASE WHEN is_delayed = false THEN 1 ELSE 0 END) as ordres_dans_les_temps,
        ROUND(SUM(CASE WHEN is_delayed = false THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as taux_respect_delais,
        ROUND(AVG(delay_days), 2) as retard_moyen_jours
    FROM maintenance_unified 
    WHERE status = 'Termin√©' AND completion_date IS NOT NULL
""")

print("üéØ Taux de respect des d√©lais global :")
display(delay_kpi)

# Taux de respect par priorit√©
delay_by_priority = spark.sql("""
    SELECT 
        priority,
        COUNT(*) as total_ordres,
        ROUND(SUM(CASE WHEN is_delayed = false THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as taux_respect_delais
    FROM maintenance_unified 
    WHERE status = 'Termin√©' AND completion_date IS NOT NULL
    GROUP BY priority
    ORDER BY taux_respect_delais DESC
""")

print("\nüìà Taux de respect par priorit√© :")
display(delay_by_priority)

# Graphique
delay_data = delay_by_priority.toPandas()
plt.figure(figsize=(10, 6))
plt.bar(delay_data['priority'], delay_data['taux_respect_delais'], color=['red', 'orange', 'yellow', 'green'])
plt.title('Taux de respect des d√©lais par priorit√©', fontsize=14, fontweight='bold')
plt.xlabel('Priorit√©')
plt.ylabel('Taux de respect (%)')
plt.ylim(0, 100)
for i, v in enumerate(delay_data['taux_respect_delais']):
    plt.text(i, v + 1, f'{v}%', ha='center', fontweight='bold')
plt.tight_layout()
plt.show()

# COMMAND ----------

# DBTITLE 1,‚è±Ô∏è KPI 2 : MTTR (Mean Time To Repair)
# Calcul du MTTR (Mean Time To Repair)
print("‚è±Ô∏è KPI 2 : MTTR (MEAN TIME TO REPAIR)")
print("=" * 45)

# MTTR global
mttr_global = spark.sql("""
    SELECT 
        ROUND(AVG(duration_hours), 2) as mttr_heures,
        ROUND(AVG(duration_hours) / 24, 2) as mttr_jours,
        COUNT(*) as nombre_interventions
    FROM maintenance_unified 
    WHERE intervention_id IS NOT NULL AND duration_hours IS NOT NULL
""")

print("üéØ MTTR global :")
display(mttr_global)

# MTTR par type d'√©quipement
mttr_by_equipment_type = spark.sql("""
    SELECT 
        equipment_type,
        ROUND(AVG(duration_hours), 2) as mttr_heures,
        COUNT(*) as nombre_interventions
    FROM maintenance_unified 
    WHERE intervention_id IS NOT NULL AND duration_hours IS NOT NULL
    GROUP BY equipment_type
    ORDER BY mttr_heures DESC
""")

print("\nüîß MTTR par type d'√©quipement :")
display(mttr_by_equipment_type)

# MTTR par type d'intervention
mttr_by_intervention_type = spark.sql("""
    SELECT 
        intervention_type,
        ROUND(AVG(duration_hours), 2) as mttr_heures,
        COUNT(*) as nombre_interventions
    FROM maintenance_unified 
    WHERE intervention_id IS NOT NULL AND duration_hours IS NOT NULL
    GROUP BY intervention_type
    ORDER BY mttr_heures DESC
""")

print("\nüîç MTTR par type d'intervention :")
display(mttr_by_intervention_type)

# COMMAND ----------

# DBTITLE 1,üí∞ KPI 3 : Co√ªts de maintenance
# Analyse des co√ªts de maintenance
print("üí∞ KPI 3 : CO√õTS DE MAINTENANCE")
print("=" * 40)

# Co√ªts globaux
cost_kpi = spark.sql("""
    SELECT 
        ROUND(SUM(total_cost), 2) as cout_total,
        ROUND(AVG(total_cost), 2) as cout_moyen_intervention,
        COUNT(*) as nombre_interventions,
        ROUND(SUM(total_cost) / COUNT(DISTINCT equipment_id), 2) as cout_moyen_par_equipement
    FROM maintenance_unified 
    WHERE intervention_id IS NOT NULL AND total_cost IS NOT NULL
""")

print("üéØ Co√ªts globaux de maintenance :")
display(cost_kpi)

# Co√ªt par type d'√©quipement
cost_by_equipment_type = spark.sql("""
    SELECT 
        equipment_type,
        ROUND(SUM(total_cost), 2) as cout_total,
        ROUND(AVG(total_cost), 2) as cout_moyen,
        COUNT(*) as nombre_interventions
    FROM maintenance_unified 
    WHERE intervention_id IS NOT NULL AND total_cost IS NOT NULL
    GROUP BY equipment_type
    ORDER BY cout_total DESC
""")

print("\nüè≠ Co√ªt par type d'√©quipement :")
display(cost_by_equipment_type)

# Co√ªt par criticit√©
cost_by_criticality = spark.sql("""
    SELECT 
        criticality,
        ROUND(SUM(total_cost), 2) as cout_total,
        ROUND(AVG(total_cost), 2) as cout_moyen,
        COUNT(*) as nombre_interventions
    FROM maintenance_unified 
    WHERE intervention_id IS NOT NULL AND total_cost IS NOT NULL
    GROUP BY criticality
    ORDER BY cout_total DESC
""")

print("\n‚ö†Ô∏è Co√ªt par criticit√© :")
display(cost_by_criticality)

# Graphique des co√ªts par type d'√©quipement
cost_data = cost_by_equipment_type.toPandas()
plt.figure(figsize=(12, 6))
plt.bar(cost_data['equipment_type'], cost_data['cout_total'], color='lightcoral')
plt.title('Co√ªt total de maintenance par type d\'\u00e9quipement', fontsize=14, fontweight='bold')
plt.xlabel('Type d\'\u00e9quipement')
plt.ylabel('Co√ªt total (‚Ç¨)')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# COMMAND ----------

# DBTITLE 1,üë• KPI 4 : Performance des techniciens
# Analyse de la performance des techniciens
print("üë• KPI 4 : PERFORMANCE DES TECHNICIENS")
print("=" * 45)

# Performance globale par technicien
technician_performance = spark.sql("""
    SELECT 
        assigned_technician,
        COUNT(DISTINCT work_order_id) as nombre_ordres_traites,
        ROUND(AVG(CASE WHEN is_delayed = false THEN 1.0 ELSE 0.0 END) * 100, 2) as taux_respect_delais,
        ROUND(AVG(CAST(duration_hours AS DOUBLE)), 2) as duree_moyenne_intervention,
        ROUND(SUM(total_cost), 2) as cout_total_interventions
    FROM maintenance_unified 
    WHERE status = 'Termin√©' AND assigned_technician IS NOT NULL
    GROUP BY assigned_technician
    ORDER BY nombre_ordres_traites DESC
""")

print("üéØ Performance par technicien :")
display(technician_performance)

# Efficacit√© par technicien (co√ªt par heure)
technician_efficiency = spark.sql("""
    SELECT 
        assigned_technician,
        ROUND(AVG(total_cost / CAST(duration_hours AS DOUBLE)), 2) as cout_par_heure,
        COUNT(*) as nombre_interventions
    FROM maintenance_unified 
    WHERE intervention_id IS NOT NULL 
        AND CAST(duration_hours AS DOUBLE) > 0 
        AND total_cost IS NOT NULL
        AND assigned_technician IS NOT NULL
    GROUP BY assigned_technician
    ORDER BY cout_par_heure ASC
""")

print("\n‚è±Ô∏è Efficacit√© par technicien (co√ªt/heure) :")
display(technician_efficiency)

# Graphique de performance
perf_data = technician_performance.toPandas()
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Nombre d'ordres trait√©s
ax1.bar(perf_data['assigned_technician'], perf_data['nombre_ordres_traites'], color='skyblue')
ax1.set_title('Nombre d\'ordres trait√©s par technicien')
ax1.set_xlabel('Technicien')
ax1.set_ylabel('Nombre d\'ordres')
ax1.tick_params(axis='x', rotation=45)

# Taux de respect des d√©lais
ax2.bar(perf_data['assigned_technician'], perf_data['taux_respect_delais'], color='lightgreen')
ax2.set_title('Taux de respect des d√©lais par technicien')
ax2.set_xlabel('Technicien')
ax2.set_ylabel('Taux de respect (%)')
ax2.set_ylim(0, 100)
ax2.tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# COMMAND ----------

# DBTITLE 1,üìâ KPI 5 : Analyse de la maintenance pr√©ventive vs corrective
# Analyse maintenance pr√©ventive vs corrective
print("üìâ KPI 5 : MAINTENANCE PR√âVENTIVE VS CORRECTIVE")
print("=" * 55)

# R√©partition pr√©ventive vs corrective
maintenance_type_analysis = spark.sql("""
    SELECT 
        work_order_type,
        COUNT(*) as nombre_ordres,
        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as pourcentage,
        ROUND(AVG(total_cost), 2) as cout_moyen,
        ROUND(AVG(duration_hours), 2) as duree_moyenne
    FROM maintenance_unified 
    WHERE work_order_type IN ('Maintenance pr√©ventive', 'Maintenance corrective')
        AND intervention_id IS NOT NULL
    GROUP BY work_order_type
    ORDER BY nombre_ordres DESC
""")

print("üéØ R√©partition maintenance pr√©ventive vs corrective :")
display(maintenance_type_analysis)

# Impact de la maintenance pr√©ventive sur les pannes
preventive_impact = spark.sql("""
    SELECT 
        equipment_id,
        equipment_name,
        SUM(CASE WHEN work_order_type = 'Maintenance pr√©ventive' THEN 1 ELSE 0 END) as nb_preventive,
        SUM(CASE WHEN work_order_type = 'Maintenance corrective' THEN 1 ELSE 0 END) as nb_corrective,
        ROUND(
            SUM(CASE WHEN work_order_type = 'Maintenance pr√©ventive' THEN 1 ELSE 0 END) * 1.0 /
            NULLIF(SUM(CASE WHEN work_order_type = 'Maintenance corrective' THEN 1 ELSE 0 END), 0), 2
        ) as ratio_preventive_corrective
    FROM maintenance_unified 
    WHERE work_order_type IN ('Maintenance pr√©ventive', 'Maintenance corrective')
    GROUP BY equipment_id, equipment_name
    HAVING SUM(CASE WHEN work_order_type = 'Maintenance corrective' THEN 1 ELSE 0 END) > 0
    ORDER BY ratio_preventive_corrective DESC
""")

print("\nüîß Impact de la maintenance pr√©ventive (Top 10) :")
display(preventive_impact.limit(10))

# Graphique en secteurs
maint_data = maintenance_type_analysis.toPandas()
plt.figure(figsize=(10, 8))
plt.pie(maint_data['nombre_ordres'], labels=maint_data['work_order_type'], autopct='%1.1f%%', startangle=90)
plt.title('R√©partition Maintenance Pr√©ventive vs Corrective', fontsize=14, fontweight='bold')
plt.axis('equal')
plt.show()

# COMMAND ----------

# DBTITLE 1,üìä Dashboard KPI - R√©sum√© ex√©cutif
# Cr√©ation d'un dashboard r√©sum√© avec tous les KPI principaux
print("üìä DASHBOARD KPI - R√âSUM√â EX√âCUTIF")
print("=" * 50)

# Collecte de tous les KPI principaux
executive_summary = spark.sql("""
    WITH kpi_data AS (
        SELECT 
            COUNT(DISTINCT equipment_id) as total_equipements,
            COUNT(DISTINCT work_order_id) as total_ordres_travail,
            COUNT(DISTINCT intervention_id) as total_interventions,
            
            -- Taux de respect des d√©lais
            ROUND(
                SUM(CASE WHEN is_delayed = false AND status = 'Termin√©' THEN 1 ELSE 0 END) * 100.0 / 
                NULLIF(SUM(CASE WHEN status = 'Termin√©' THEN 1 ELSE 0 END), 0), 2
            ) as taux_respect_delais,
            
            -- MTTR
            ROUND(AVG(duration_hours), 2) as mttr_heures,
            
            -- Co√ªts
            ROUND(SUM(total_cost), 2) as cout_total_maintenance,
            ROUND(AVG(total_cost), 2) as cout_moyen_intervention,
            
            -- Maintenance pr√©ventive vs corrective
            ROUND(
                SUM(CASE WHEN work_order_type = 'Maintenance pr√©ventive' THEN 1 ELSE 0 END) * 100.0 /
                NULLIF(SUM(CASE WHEN work_order_type IN ('Maintenance pr√©ventive', 'Maintenance corrective') THEN 1 ELSE 0 END), 0), 2
            ) as taux_maintenance_preventive
            
        FROM maintenance_unified
    )
    SELECT * FROM kpi_data
""")

print("üéØ INDICATEURS CL√âS DE PERFORMANCE :")
print("=" * 40)
display(executive_summary)

# Cr√©ation d'un r√©sum√© visuel
summary_data = executive_summary.collect()[0]

# Graphique de synth√®se
fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

# KPI 1: Taux de respect des d√©lais
ax1.pie([summary_data['taux_respect_delais'], 100-summary_data['taux_respect_delais']], 
        labels=['Dans les temps', 'En retard'], 
        colors=['green', 'red'], autopct='%1.1f%%')
ax1.set_title('Taux de respect des d√©lais')

# KPI 2: R√©partition des co√ªts (exemple)
ax2.bar(['Co√ªt total'], [summary_data['cout_total_maintenance']], color='orange')
ax2.set_title('Co√ªt total de maintenance')
ax2.set_ylabel('Co√ªt (‚Ç¨)')

# KPI 3: MTTR
ax3.bar(['MTTR'], [summary_data['mttr_heures']], color='blue')
ax3.set_title('MTTR (Mean Time To Repair)')
ax3.set_ylabel('Heures')

# KPI 4: Maintenance pr√©ventive vs corrective
ax4.pie([summary_data['taux_maintenance_preventive'], 100-summary_data['taux_maintenance_preventive']], 
        labels=['Pr√©ventive', 'Corrective'], 
        colors=['lightblue', 'lightcoral'], autopct='%1.1f%%')
ax4.set_title('R√©partition Pr√©ventive/Corrective')

plt.suptitle('Dashboard KPI - Maintenance GMAO', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.show()

print("\n‚úÖ Dashboard KPI cr√©√© avec succ√®s !")
print("üöÄ Analyse compl√®te de la maintenance termin√©e !")

# COMMAND ----------

# DBTITLE 1,üéØ Query 1: KPI Summary Card
# MAGIC %md
# MAGIC ---
# MAGIC # üéâ Conclusion de l'atelier
# MAGIC
# MAGIC ## üéØ Ce que vous avez appris
# MAGIC
# MAGIC F√©licitations ! Vous avez r√©alis√© votre premier projet complet sur Databricks. Voici ce que vous ma√Ætrisez maintenant :
# MAGIC
# MAGIC ### üì• **Ingestion de donn√©es**
# MAGIC * Utilisation d'**Autoloader** pour charger automatiquement des fichiers CSV
# MAGIC * Cr√©ation de tables **Delta Lake** pour un stockage optimis√©
# MAGIC * Gestion des sch√©mas et des checkpoints
# MAGIC
# MAGIC ### üîç **Exploration et analyse**
# MAGIC * Exploration interactive des donn√©es avec **PySpark**
# MAGIC * Cr√©ation de visualisations avec **Matplotlib**
# MAGIC * Analyse statistique des donn√©es de maintenance
# MAGIC
# MAGIC ### üîß **Transformation de donn√©es**
# MAGIC * Nettoyage et enrichissement des donn√©es
# MAGIC * Jointures complexes entre plusieurs tables
# MAGIC * Cr√©ation de colonnes calcul√©es et de cat√©gories
# MAGIC
# MAGIC ### üìä **Calcul d'indicateurs m√©tier**
# MAGIC * **KPI de maintenance** : MTTR, taux de respect des d√©lais, co√ªts
# MAGIC * **Analyse de performance** des techniciens
# MAGIC * **Dashboards visuels** pour le pilotage
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ## üöÄ Prochaines √©tapes
# MAGIC
# MAGIC ### Pour aller plus loin avec Databricks :
# MAGIC
# MAGIC * **ü§ñ Machine Learning** : Pr√©dire les pannes avec MLflow
# MAGIC * **üìà Dashboards avanc√©s** : Cr√©er des tableaux de bord interactifs
# MAGIC * **‚è∞ Automatisation** : Planifier des jobs de traitement
# MAGIC * **üîÑ Streaming** : Traiter des donn√©es en temps r√©el
# MAGIC * **üìä SQL Analytics** : Requ√™tes SQL avanc√©es sur vos donn√©es
# MAGIC
# MAGIC ### Ressources utiles :
# MAGIC * [Documentation Databricks](https://docs.databricks.com/)
# MAGIC * [Databricks Academy](https://academy.databricks.com/)
# MAGIC * [Communaut√© Databricks](https://community.databricks.com/)
# MAGIC
# MAGIC ---
# MAGIC
# MAGIC ## üí¨ Questions et discussion
# MAGIC
# MAGIC **Avez-vous des questions sur :**
# MAGIC * Les concepts abord√©s dans cet atelier ?
# MAGIC * L'application √† vos cas d'usage sp√©cifiques ?
# MAGIC * Les fonctionnalit√©s avanc√©es de Databricks ?
# MAGIC
# MAGIC **Merci d'avoir particip√© √† cet atelier Databricks - GMAO !** üéâ
