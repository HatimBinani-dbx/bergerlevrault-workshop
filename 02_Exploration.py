# Databricks notebook source
# MAGIC %md
# MAGIC ---
# MAGIC # ğŸ” Ã‰tape 3 : Exploration des donnÃ©es GMAO
# MAGIC
# MAGIC Maintenant que nos donnÃ©es sont ingÃ©rÃ©es, explorons-les pour mieux comprendre notre parc d'Ã©quipements et nos activitÃ©s de maintenance.
# MAGIC
# MAGIC ## ğŸ¯ Ce que nous allons dÃ©couvrir :
# MAGIC * La structure et la qualitÃ© de nos donnÃ©es
# MAGIC * La rÃ©partition des Ã©quipements par type et localisation
# MAGIC * L'Ã©tat des ordres de travail
# MAGIC * Les tendances de maintenance

# COMMAND ----------

catalog = 'gmao_catalog'
schema = 'gmao_schema'
volume = 'gmao_volume'
data_path = f"/Volumes/{catalog}/{schema}/{volume}"

equipments_table = spark.read.table(f"{catalog}.{schema}.equipments")
work_orders_table = spark.read.table(f"{catalog}.{schema}.work_orders")
interventions_table = spark.read.table(f"{catalog}.{schema}.interventions")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ” Equipements

# COMMAND ----------

# DBTITLE 1,ğŸ“Š Vue d'ensemble des Ã©quipements
from pyspark.sql.functions import desc

# Exploration des donnÃ©es d'Ã©quipements
print("ğŸ­ ANALYSE DES Ã‰QUIPEMENTS")
print("=" * 40)

# Affichage des premiÃ¨res lignes
print("ğŸ“‹ AperÃ§u des donnÃ©es :")
display(equipments_table.limit(5))

# Statistiques gÃ©nÃ©rales
print(f"\nğŸ“Š Nombre total d'Ã©quipements : {equipments_table.count()}")
print(f"ğŸ“Š Nombre de colonnes : {len(equipments_table.columns)}")

# RÃ©partition par type d'Ã©quipement
print("\nğŸ”§ RÃ©partition par type d'Ã©quipement :")
equipment_types_count = equipments_table.groupBy("equipment_type").count().orderBy(desc("count"))
display(equipment_types_count)

# COMMAND ----------

# DBTITLE 1,ğŸ“ Analyse de la localisation des Ã©quipements
# Analyse de la rÃ©partition gÃ©ographique des Ã©quipements
import matplotlib.pyplot as plt

print("ğŸ“ ANALYSE DE LOCALISATION")
print("=" * 35)

# RÃ©partition par localisation
location_stats = equipments_table.groupBy("location").count().orderBy(desc("count")).toPandas()

# Graphique en barres
plt.figure(figsize=(12, 6))
plt.bar(location_stats['location'], location_stats['count'], color='steelblue')
plt.title('RÃ©partition des Ã©quipements par localisation', fontsize=14, fontweight='bold')
plt.xlabel('Localisation')
plt.ylabel('Nombre d\'\u00e9quipements')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

print(f"\nğŸ“ Nombre de localisations diffÃ©rentes : {location_stats.shape[0]}")
print(f"ğŸ¯ Localisation avec le plus d'Ã©quipements : {location_stats.iloc[0]['location']} ({location_stats.iloc[0]['count']} Ã©quipements)")

# COMMAND ----------

# DBTITLE 1,âš ï¸ Analyse de la criticitÃ© et du statut
# Analyse de la criticitÃ© et du statut des Ã©quipements
print("âš ï¸ ANALYSE DE CRITICITÃ‰ ET STATUT")
print("=" * 40)

# RÃ©partition par criticitÃ©
criticality_stats = equipments_table.groupBy("criticality").count().orderBy(desc("count"))
print("ğŸ“Š RÃ©partition par criticitÃ© :")
display(criticality_stats)

# RÃ©partition par statut
status_stats = equipments_table.groupBy("status").count().orderBy(desc("count"))
print("\nğŸ”„ RÃ©partition par statut :")
display(status_stats)

# Analyse croisÃ©e criticitÃ© vs statut
print("\nğŸ” Analyse croisÃ©e criticitÃ© vs statut :")
cross_analysis = equipments_table.groupBy("criticality", "status").count().orderBy("criticality", desc("count"))
display(cross_analysis)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ” Work Orders

# COMMAND ----------

# DBTITLE 1,ğŸ“‹ Exploration des ordres de travail
# Exploration des ordres de travail
print("ğŸ“‹ ANALYSE DES ORDRES DE TRAVAIL")
print("=" * 40)

# Vue d'ensemble
print("ğŸ“‹ AperÃ§u des ordres de travail :")
display(work_orders_table.limit(5))

print(f"\nğŸ“Š Nombre total d'ordres de travail : {work_orders_table.count()}")

# RÃ©partition par type d'ordre de travail
print("\nğŸ”§ RÃ©partition par type d'ordre de travail :")
work_order_types_count = work_orders_table.groupBy("work_order_type").count().orderBy(desc("count"))
display(work_order_types_count)

# RÃ©partition par statut
print("\nğŸ”„ RÃ©partition par statut :")
work_order_status_count = work_orders_table.groupBy("status").count().orderBy(desc("count"))
display(work_order_status_count)

# COMMAND ----------

# DBTITLE 1,ğŸ“ˆ Analyse temporelle des ordres de travail
from pyspark.sql.functions import col, date_format, weekofyear

# Analyse temporelle des ordres de travail
print("ğŸ“ˆ ANALYSE TEMPORELLE")
print("=" * 25)

# Ajout de colonnes temporelles pour l'analyse
work_orders_with_time = work_orders_table.withColumn(
    "creation_month", date_format(col("creation_date"), "yyyy-MM")
).withColumn(
    "creation_week", weekofyear(col("creation_date"))
)

# Tendance mensuelle de crÃ©ation d'ordres de travail
monthly_trend = work_orders_with_time.groupBy("creation_month").count().orderBy("creation_month")
print("ğŸ“… Tendance mensuelle de crÃ©ation d'ordres de travail :")
display(monthly_trend)

# Graphique de tendance
monthly_data = monthly_trend.toPandas()
plt.figure(figsize=(12, 6))
plt.plot(monthly_data['creation_month'], monthly_data['count'], marker='o', linewidth=2, markersize=8)
plt.title('Tendance mensuelle des ordres de travail', fontsize=14, fontweight='bold')
plt.xlabel('Mois')
plt.ylabel('Nombre d\'ordres de travail')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

print(f"\nğŸ“Š Mois le plus chargÃ© : {monthly_data.loc[monthly_data['count'].idxmax(), 'creation_month']} ({monthly_data['count'].max()} ordres)")

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ” Interventions

# COMMAND ----------

# DBTITLE 1,ğŸ”§ Exploration des interventions
from pyspark.sql.functions import avg, min, max, stddev
# Exploration des interventions
print("ğŸ”§ ANALYSE DES INTERVENTIONS")
print("=" * 35)

# Vue d'ensemble
print("ğŸ“‹ AperÃ§u des interventions :")
display(interventions_table.limit(5))

print(f"\nğŸ“Š Nombre total d'interventions : {interventions_table.count()}")

# RÃ©partition par type d'intervention
print("\nğŸ”§ RÃ©partition par type d'intervention :")
intervention_types_count = interventions_table.groupBy("intervention_type").count().orderBy(desc("count"))
display(intervention_types_count)

# Statistiques sur les durÃ©es
print("\nâ±ï¸ Statistiques sur les durÃ©es d'intervention :")

# Statistiques sur les durÃ©es
duration_stats = interventions_table.select(

    avg("duration_hours").alias("duree_moyenne"),
    min("duration_hours").alias("duree_min"),
    max("duration_hours").alias("duree_max"),
    stddev("duration_hours").alias("ecart_type")
)
display(duration_stats)

# COMMAND ----------

# DBTITLE 1,ğŸ’° Analyse des coÃ»ts de maintenance
from pyspark.sql.functions import sum, avg, count, col, desc

# Analyse des coÃ»ts de maintenance
print("ğŸ’° ANALYSE DES COÃ›TS")
print("=" * 25)

# Statistiques gÃ©nÃ©rales des coÃ»ts
cost_stats = interventions_table.select(

    sum("parts_cost").alias("cout_total_pieces"),
    sum("labor_cost").alias("cout_total_main_oeuvre"),
    avg("parts_cost").alias("cout_moyen_pieces"),
    avg("labor_cost").alias("cout_moyen_main_oeuvre")
)

print("ğŸ“Š Statistiques des coÃ»ts :")
display(cost_stats)

# CoÃ»t total par intervention
interventions_with_total_cost = interventions_table.withColumn(
    "total_cost", (col("parts_cost").cast("float")) + (col("labor_cost").cast("float"))
)

# Top 10 des interventions les plus coÃ»teuses
print("\nğŸ’¸ Top 10 des interventions les plus coÃ»teuses :")
top_costly_interventions = interventions_with_total_cost.select(
    "intervention_id", "intervention_type", "parts_cost", "labor_cost", "total_cost"
).orderBy(desc("total_cost")).limit(10)
display(top_costly_interventions)

# RÃ©partition des coÃ»ts par type d'intervention
print("\nğŸ“ˆ CoÃ»t moyen par type d'intervention :")
cost_by_type = interventions_with_total_cost.groupBy("intervention_type").agg(
    avg("total_cost").alias("cout_moyen"),
    count("*").alias("nombre_interventions")
).orderBy(desc("cout_moyen"))
display(cost_by_type)

# COMMAND ----------

# MAGIC %md
# MAGIC ## ğŸ” ?

# COMMAND ----------

