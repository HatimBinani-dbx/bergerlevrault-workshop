# Databricks notebook source
# MAGIC %md
# MAGIC ---
# MAGIC # ğŸ“¥ Ã‰tape 2 : Ingestion des donnÃ©es avec Autoloader
# MAGIC
# MAGIC **Autoloader** est une fonctionnalitÃ© Databricks qui permet de charger automatiquement des fichiers dÃ¨s qu'ils arrivent dans un rÃ©pertoire. C'est parfait pour traiter des donnÃ©es en continu !
# MAGIC
# MAGIC ## ğŸ¯ Ce que nous allons faire :
# MAGIC * Configurer Autoloader pour lire nos fichiers CSV
# MAGIC * CrÃ©er des tables Delta pour stocker nos donnÃ©es
# MAGIC * VÃ©rifier que l'ingestion fonctionne correctement

# COMMAND ----------

catalog = 'gmao_catalog'
schema = 'gmao_schema'
volume = 'gmao_volume'
data_path = f"/Volumes/{catalog}/{schema}/{volume}"

# COMMAND ----------

# DBTITLE 1,ğŸ”§ Configuration d'Autoloader pour les Ã©quipements
# Configuration d'Autoloader pour charger les donnÃ©es d'Ã©quipements
# Autoloader surveille automatiquement le rÃ©pertoire et charge les nouveaux fichiers

# Lecture avec Autoloader
equipments_df = (spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "csv")
    .option("cloudFiles.schemaLocation", f"{data_path}/schemas/equipments")
    .option("header", "true")
    .option("inferSchema", "true")
    .load(data_path + "/equipments")
)

print("âœ… Configuration Autoloader pour les Ã©quipements terminÃ©e")
print(f"ğŸ“Š SchÃ©ma dÃ©tectÃ© : {len(equipments_df.columns)} colonnes")
print("ğŸ”„ Autoloader surveille maintenant le rÃ©pertoire pour les nouveaux fichiers")

# COMMAND ----------

# DBTITLE 1,ğŸ’¾ Sauvegarde des Ã©quipements en table Delta
# Sauvegarde des donnÃ©es d'Ã©quipements en format Delta
# Delta Lake offre des fonctionnalitÃ©s avancÃ©es : versioning, time travel, ACID transactions

equipments_table_name = f"{catalog}.{schema}.equipments"

# Ã‰criture en streaming vers une table Delta Unity Catalog
equipments_query = (equipments_df.writeStream
    .format("delta")
    .outputMode("append")
    .option("checkpointLocation", f"{data_path}/checkpoints/equipments")
    .trigger(once=True)
    .toTable(equipments_table_name)
)

# Attendre la fin du traitement
equipments_query.awaitTermination()

print("âœ… Table Delta 'equipments' crÃ©Ã©e avec succÃ¨s")
print(f"ğŸ“ Emplacement : {equipments_table_name}")
print("ğŸ¯ Les donnÃ©es sont maintenant disponibles pour l'analyse !")

# COMMAND ----------

equipments_table = spark.read.table(f"{catalog}.{schema}.equipments")
display(equipments_table)

# COMMAND ----------

# DBTITLE 1,ğŸ”§ Configuration d'Autoloader pour les ordres de travail
from pyspark.sql.types import *

# Configuration d'Autoloader pour les ordres de travail
work_orders_df = (spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "csv")
    .option("cloudFiles.schemaLocation", f"{data_path}/schemas/work_orders")
    .option("header", "true")
    .option("inferSchema", "true")
    .load(data_path + "/work_orders")
)

print("âœ… Configuration Autoloader pour les ordres de travail terminÃ©e")
print(f"ğŸ“Š SchÃ©ma dÃ©tectÃ© : {len(work_orders_df.columns)} colonnes")

# COMMAND ----------

# DBTITLE 1,ğŸ’¾ Sauvegarde des ordres de travail en table Delta
# Sauvegarde des ordres de travail en format Delta

work_orders_table_name = f"{catalog}.{schema}.work_orders"

work_orders_query = (work_orders_df.writeStream
    .format("delta")
    .outputMode("append")
    .option("checkpointLocation", f"{data_path}/checkpoints/work_orders")
    .trigger(once=True)
    .toTable(work_orders_table_name)
)

work_orders_query.awaitTermination()

print("âœ… Table Delta 'work_orders' crÃ©Ã©e avec succÃ¨s")
print(f"ğŸ“ Emplacement : {work_orders_table_name}")

# COMMAND ----------

work_orders_table = spark.read.table(f"{catalog}.{schema}.work_orders")
display(work_orders_table)

# COMMAND ----------

# DBTITLE 1,ğŸ”§ Configuration d'Autoloader pour les interventions
# Configuration d'Autoloader pour les interventions

interventions_df = (spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "csv")
    .option("cloudFiles.schemaLocation", f"{data_path}/schemas/interventions")
    .option("header", "true")
    .option("inferSchema", "true")
    .load(data_path + "/interventions")
)

print("âœ… Configuration Autoloader pour les interventions terminÃ©e")
print(f"ğŸ“Š SchÃ©ma dÃ©tectÃ© : {len(interventions_df.columns)} colonnes")

# COMMAND ----------

# DBTITLE 1,ğŸ’¾ Sauvegarde des interventions en table Delta
# Sauvegarde des interventions en format Delta

interventions_table_name = f"{catalog}.{schema}.interventions"

interventions_query = (interventions_df.writeStream
    .format("delta")
    .outputMode("append")
    .option("checkpointLocation", f"{data_path}/checkpoints/interventions")
    .trigger(once=True)
    .toTable(interventions_table_name)
)

interventions_query.awaitTermination()

print("âœ… Table Delta 'interventions' crÃ©Ã©e avec succÃ¨s")
print(f"ğŸ“ Emplacement : {interventions_table_name}")

# COMMAND ----------

interventions_table = spark.read.table(f"{catalog}.{schema}.interventions")
display(interventions_table)

# COMMAND ----------

# DBTITLE 1,âœ… VÃ©rification de l'ingestion
# VÃ©rification que toutes les tables ont Ã©tÃ© crÃ©Ã©es correctement

# Lecture des tables Delta crÃ©Ã©es
print("\nğŸ‰ Toutes les donnÃ©es GMAO ont Ã©tÃ© ingÃ©rÃ©es avec succÃ¨s !")
print("ğŸ“Š RÃ©sumÃ© de l'ingestion :")
print(f"  ğŸ­ Ã‰quipements : {equipments_table.count()} lignes")
print(f"  ğŸ“‹ Ordres de travail : {work_orders_table.count()} lignes")
print(f"  ğŸ”§ Interventions : {interventions_table.count()} lignes")

print("\nâœ… Ingestion terminÃ©e avec succÃ¨s !")
print("ğŸš€ PrÃªt pour l'exploration des donnÃ©es...")